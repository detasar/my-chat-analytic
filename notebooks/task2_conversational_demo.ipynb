{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Conversational Demo\n",
    "\n",
    "This notebook demonstrates **quick solutions** to transform our zero-shot pipeline into a more **conversational** system without changing the `facebook/bart-large-mnli` model.\n",
    "\n",
    "## Overview\n",
    "1. We'll initialize the same database and zero-shot analyzer.\n",
    "2. We'll import our new **`ConversationalManager`** (from `src/conversational_manager`) to handle context windows.\n",
    "3. We'll simulate a conversation, step by step, showing how we can pass the last `N` utterances as context.\n",
    "4. We'll store results in the same `logs` table, so you can see how both Task 1 and Task 2 logs might coexist.\n",
    "\n",
    "## Prerequisites\n",
    "- Run `pip install -r requirements.txt`.\n",
    "- Confirm your `config/model_config.ini` is configured as needed (e.g., `device=cpu` or `cuda`).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import asyncio\n",
    "import configparser\n",
    "import json\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# We'll import the classes from our src/ package:\n",
    "from src.db import Database\n",
    "from src.logger import AsyncLogger\n",
    "from src.sentiment_intent_analyzer import ZeroShotAnalyzer\n",
    "from src.conversational_manager import ConversationalManager\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Read configuration & initialize DB\n",
    "We'll do the same steps as Task 1, but inside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "config = configparser.ConfigParser()\n",
    "config_path = os.path.join(\"..\", \"config\", \"model_config.ini\")\n",
    "\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"model_config.ini not found at {config_path}\")\n",
    "\n",
    "config.read(config_path)\n",
    "\n",
    "model_name = config[\"DEFAULT\"].get(\"model_name\", \"facebook/bart-large-mnli\")\n",
    "device = config[\"DEFAULT\"].get(\"device\", \"cpu\")\n",
    "db_url = config[\"DEFAULT\"].get(\"db_url\", \"sqlite+aiosqlite:///./logs.db\")\n",
    "\n",
    "print(\"Model Name:\", model_name)\n",
    "print(\"Device:\", device)\n",
    "print(\"DB URL:\", db_url)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Initialize Database & Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "db = Database(db_url)\n",
    "await db.init_db()\n",
    "logger = AsyncLogger(db)\n",
    "\n",
    "analyzer = ZeroShotAnalyzer(\n",
    "    model_name=model_name,\n",
    "    device=device\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Create our ConversationalManager\n",
    "We'll set a small `context_window` of **2**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "conv_manager = ConversationalManager(\n",
    "    analyzer=analyzer,\n",
    "    logger=logger,\n",
    "    context_window=2\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Load a conversation (the same or a different JSON) & simulate multi-turn\n",
    "We'll re-use `conversation.json` from the parent `data` folder, or you can create a new custom conversation. Each turn we pass to `conv_manager.process_utterance`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_path = os.path.join(\"..\", \"data\", \"conversation.json\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"conversation.json not found at {data_path}\")\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    conv_data = json.load(f)\n",
    "\n",
    "conversation = conv_data.get(\"conversation\", [])\n",
    "print(f\"Loaded {len(conversation)} utterances from conversation.json.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's process each utterance with context\n",
    "We'll do it step by step and display the classification results.\n",
    "\n",
    "> In a real conversational system, you'd do this **interactively** each time a new user message arrives."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "async def run_conversational_demo(conversation):\n",
    "    results = []\n",
    "    for turn in conversation:\n",
    "        role = turn.get(\"role\", \"unknown\")\n",
    "        text = turn.get(\"text\", \"\")\n",
    "\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Process with context-based approach\n",
    "        analysis = await conv_manager.process_utterance(role, text)\n",
    "        results.append(analysis)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = await run_conversational_demo(conversation)\n",
    "\n",
    "print(\"Finished contextual classification!\")\n",
    "print(f\"Logged {len(results)} messages in total.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Inspect the results\n",
    "We stored them in `logs.db` again, but let's just see them in-memory too."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for idx, r in enumerate(results[:5]):\n",
    "    display(Markdown(f\"**Utterance {idx+1}:** Role: {r['role']} | Text: ...<truncated>...\\n\\n\"\n",
    "                    f\"- Sentiment: {r['sentiment']}\\n\"\n",
    "                    f\"- Intent: {r['intent']}\\n\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "We've now demonstrated how the same **BART MNLI** zero-shot classifier can be turned into a **conversational** system by:\n",
    "\n",
    "1. **Maintaining short memory** (context window) of previous utterances.\n",
    "2. Combining them with the current text for classification.\n",
    "3. Logging results to the same async DB.\n",
    "\n",
    ">  This approach can be extended by adding rule-based or state-machine logic, session IDs, or other domain-specific logic.\n",
    "\n",
    "**Thank you**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
